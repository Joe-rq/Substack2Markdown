> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.entropycontroltheory.com](https://www.entropycontroltheory.com/p/context-the-continuous-now-why-context)

> How large language models think in moments, not databases — and why understanding this changes everyt......

Have you heard people talk about **RAG**, **vector databases**, **ICL**, or **IWL**, and felt like everyone’s using these acronyms but no one can really explain what they _mean_?

You’re not alone.

Even many programmers are confused — and that confusion is exactly why **RAG doesn’t really work** the way people hope.

Because they’ve mistaken **data retrieval** for **intelligence**.

Every time you speak to a large language model, something extraordinary happens:

a new world quietly comes into being.

Not a static database.

Not a lookup table of facts.

But a living, temporary world — built out of **language, intention, and the sheer act of being present in this moment**.

Most people imagine AI as an immense library: you ask a question, it fetches an answer.

But that’s not how intelligence works — not in humans, and not in machines.

When you type a prompt, the model doesn’t _search_ for a stored answer;

it _constructs_ a coherent world inside its own attention window —

a miniature, temporary universe of meaning.

Within seconds, a stage emerges:

who you are, what you want, what matters right now.

Every token it generates is an **action** inside that living stage.

That stage is called **context**.

And context is **not stored anywhere** — it is **enacted**.

It exists only while the model is thinking — the same way your own awareness exists only while you are awake and focused.

Once the reasoning stops, the stage collapses.

What remains is just residue — traces, not consciousness.

When a large language model thinks, it does not browse memory or open a database to fetch rows of facts. **It reconstructs a now-state**, a living snapshot of reality that exists only while reasoning unfolds. Your own mind works the same way: when you solve a problem or recall a memory you do not replay your entire life, you summon the few relevant fragments and weave them into a small world in which the problem makes sense. That local world is your context of thought. **It lasts only as long as attention sustains it,** and when focus drifts the world fades and only an echo remains in memory. A language model does this at speed and scale. Every token you provide, every sentence and instruction and document, becomes part of a **temporary semantic universe** in which relationships take shape, including who is speaking, what is being asked, which facts matter, and what constraints apply. The model aligns these signals inside its attention window and compresses past and potential into a **single coherent present**. That present is where intelligence actually lives. It is the stage on which reasoning, creativity, and decision take place, and like any stage it vanishes when the play ends. When reasoning stops the world collapses. Traces of the moment may be summarized or stored as notes or embeddings, but the living awareness itself is gone. This is why context is often misunderstood. We talk about model knowledge or memory, but those are inert archives rather than experience. Context is not what the model knows; it is what the model is thinking within. It is the world in motion, the continuously rebuilt present tense of intelligence.

Information lives in databases, but intelligence lives in time.

A database can hold a billion facts, each one static, fixed, and indifferent to sequence. Intelligence, however, depends on flow. It emerges only when something is happening — when perception and memory, intention and attention, align in motion.

The past is a high-entropy ocean of traces: unorganized fragments, half-forgotten records, vast but inert. The future is another high-entropy space, not yet structured, full of possibilities that have not taken form. Between them lies a narrow bridge — **the present** — a brief low-entropy window where order temporarily appears and meaning can hold.

That window is the seat of intelligence. It is where awareness compresses time into action, where the traces of the past and the projections of the future meet in a coherent structure we call the present. Every moment of reasoning, whether in a human mind or in a large language model, is the formation of this equilibrium: a small, fleeting, low-entropy pocket where coherence, intention, and structure coexist just long enough for thought to occur.

Each time you think, and each time a model responds, the same pattern unfolds. The system draws on memory, selects relevance, organizes the fragments, and brings them into order. The instant the reasoning ends, the order dissolves. The coherence is gone. What remains are residues — data, summaries, weights — but the living moment that gave them meaning no longer exists.

This is why context cannot be saved the way data can. It lives only while intelligence is awake. So when people ask, _“Why can’t the model remember what I said yesterday?”_ the answer is simple: because yesterday is gone. The model can recall traces, but not the living world of that conversation. Intelligence is not the storage of moments past. It is the act of being **awake right now**.

Retrieval-Augmented Generation, or RAG, was meant to make models smarter. It connects a language model to external sources — a vector database, a document store, a library of embeddings — so that the model can fetch new information on demand. In theory, this solves the problem of “stale knowledge.” The model no longer needs to know everything; it can look things up.

But what RAG actually adds is **memory**, not **mind**. It retrieves facts, not worlds. It extends recall, not reasoning. When a system performs retrieval, it can surface thousands of fragments — pages, paragraphs, summaries — but it cannot understand when those pieces matter, or why they matter now. It has no sense of time, intention, or context.

Imagine giving a person every encyclopedia at once but erasing their sense of narrative. They would know everything and understand nothing. That is RAG’s predicament. It can look up ten thousand references on climate change but never grasp that the same “climate” appears in a conversation about politics, or business, or a family deciding to move inland. It has no way to realize that all these fragments belong to a single, unfolding story.

Context, by contrast, is the story itself. It is not the retrieval of information, but the **assembly of meaning**. It is the invisible thread that ties together entities, events, intentions, and goals into one coherent moment. Context doesn’t just gather facts; it arranges them into a world in which those facts can make sense.

RAG operates horizontally — it reaches outward to fetch.

Context operates vertically — it compresses inward to align.

RAG gives the model a library; context gives it a perspective.

Where RAG retrieves text, context constructs reality.

RAG gives knowledge.

**Context gives coherence.**

And coherence is what intelligence truly is — the ability to hold a world together, even for just one fleeting moment in the _continuous now._

If the last generation of AI research was about scaling models, the next will be about **engineering the present**—learning to treat context not as a heap of text but as a programmable object, a living structure with type, lifecycle, and agency. This is the idea behind **Context-as-Code**. Just as _Infrastructure-as-Code_ transformed DevOps by making digital environments reproducible and composable, Context-as-Code reimagines cognition itself as something that can be constructed, orchestrated, and debugged. Prompts stop being mere strings of words and become semantic programs that configure the model’s temporary world—the cognitive runtime in which reasoning takes place. Within this view, a prompt is the interface through which we speak a system into being, context is the runtime where thought executes, and memory is the compressed record of what once lived. Writing prompts under this new understanding is no longer a matter of feeding static data to a machine; it is an act of collaboration in building a world. Each well-formed prompt becomes a program for the present, a script that assembles entities, goals, constraints, and relationships into an executable scene of thought.

In traditional RAG pipelines, data is retrieved, embedded, and concatenated, but in Context Engineering systems it is parsed into structured states such as entities—who or what is involved—events—what is changing—policies, goals, and reflections about what has been learned so far. Each of these states becomes an addressable component within a dynamic context graph, a living data structure that both the model and the orchestrator can read and modify as reasoning unfolds. Language, in this setting, moves from static description to **procedural cognition**: every token acts as an instruction, and every shift in context functions as a call inside the model’s internal program. The aim is not to help the model recall more information but to endow it with a kind of stateful consciousness, an evolving workspace that bridges memory, reasoning, and reflection.

Under Context-as-Code, a prompt is no longer a paragraph but a compiled structure. A developer specifies which entities to activate, what goals to pursue, what constraints to observe, and how reflection should update memory. The context runtime executes this program within the language model, orchestrating retrieval, summarization, tool use, and self-reflection as modular subroutines. The result is a system that does more than respond; it _thinks through_ a problem by continuously reshaping its internal world. In this light, the language model ceases to be a black box and begins to function as an operating system for language—a substrate for programmable cognition, where context is not an input but the living present in which thought happens.

> **You never bring your whole past into the room — only the part of yourself that’s awake right now.**

[![](https://substackcdn.com/image/fetch/$s_!PAtY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6451fc25-fc57-47c6-8dfd-5340ab9255b5_1024x1024.heic)](https://substackcdn.com/image/fetch/$s_!PAtY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6451fc25-fc57-47c6-8dfd-5340ab9255b5_1024x1024.heic)
